{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "This notebook fulfills two main purposes:\n",
    "\n",
    "1. Intake the data from the exported json files and clean it up\n",
    "2. Get an idea of wha the data holds\n",
    "\n",
    "Intaking the data requires reading each of the 1,380 individual data files, converting them into individual Pandas DataFrames and then creating one big, unified DataFrame that is easier to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions and Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maps shorthands in sample filenames to \"english\" translations of what they stand for\n",
    "fname_shorthand_dict = {\n",
    "    \"b\": \"batch_size\",\n",
    "    \"k\": \"kernel_size\",\n",
    "    \"s\": \"stride_length\",\n",
    "    \"f\": \"filter_count\"\n",
    "}\n",
    "\n",
    "\n",
    "def parse_sample_filename(fname):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        fname: string of a sample filename (no file extension)\n",
    "        \n",
    "    Returns:\n",
    "        params: characters in fname \n",
    "        vals: values corresponding to those characters\n",
    "        \n",
    "    This method is used for decoding the paramter values encoded in the filenames of samples\n",
    "    >>> parse_sample_filename(\"a1b2c10\")\n",
    "    >>> (['a', 'b', 'c'], ['1', '2', '10'])\n",
    "    \"\"\"\n",
    "    params = re.findall(\"[a-zA-Z]{1}\", fname)\n",
    "    \n",
    "    # translating param shorthand to \"English\" name\n",
    "    params = list(map(lambda x: fname_shorthand_dict[x], params))\n",
    "    \n",
    "    vals = re.findall(\"[0-9]+\", fname) \n",
    "    \n",
    "    # converting vals from strings to integers\n",
    "    vals = list(map(int, vals))\n",
    "\n",
    "    return params, vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data from Disk and Creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of json files containing profile data\n",
    "# default location is DataHonorsThesis/data/export_logs/\n",
    "data_location = \"../../data/\"\n",
    "export_logs = data_location + \"export_logs/\"\n",
    "# storing a dictionary of sample names to sample DataFrames\n",
    "# will eventually be used to extract meaningful data and create one big DataFrame\n",
    "sample_dfs = []\n",
    "\n",
    "# goes through every .json file in the data_base_path directory and turns it into a DataFrame\n",
    "# these DataFrames are then added into the sample_dfs dict for further processing\n",
    "# this can take up to a minute to run\n",
    "for fname in os.listdir(export_logs):\n",
    "    # appending the base path to the filename so it can be loaded by pandas\n",
    "    full_fname = export_logs + fname\n",
    "    \n",
    "    # only read in json\n",
    "    if full_fname.endswith(\".json\"):\n",
    "        #print(\"Reading: %s\" %fname)\n",
    "        json_raw = pd.read_json(\n",
    "            path_or_buf = full_fname,\n",
    "            orient = \"records\"\n",
    "        )\n",
    "        \n",
    "        # TODO: change this script to using functions like map, flatmap, etc.\n",
    "        \n",
    "        # extract column names from first row of first col in loaded json\n",
    "        col_names = [json_raw.cols[0][i][\"id\"] for i in range(len(json_raw.cols[0]))]\n",
    "        \n",
    "        # split the single row loaded from the json into 1 array per row\n",
    "        rows = [json_raw.rows[0][i][\"c\"] for i in range(len(json_raw.rows[0]))]\n",
    "        # transform \"rows\" (current lists of dicts) into lists of values\n",
    "        rows = [[row[i][\"v\"] for i in range(len(row))] for row in rows]\n",
    "        #print(col_names)\n",
    "        #print(rows[0])\n",
    "        \n",
    "        # now that rows and columns have been extracted, turn them into a 'real' DataFrame\n",
    "        formatted_df = pd.DataFrame(data=rows, columns=col_names)\n",
    "        \n",
    "        # add the values for experimental parameters (e.g. strides, ) as new columns to our dataframe\n",
    "        params, vals = parse_sample_filename(re.sub(\".json\", \"\", fname))\n",
    "        \n",
    "        #print(params)\n",
    "        formatted_df[params] = pd.DataFrame([vals], index = formatted_df.index)\n",
    "        \n",
    "        # adding our new DataFrame to the sample_dfs dict with key = fname\n",
    "        # (sub to cut off the '.json' from the filename)\n",
    "        sample_dfs.append(formatted_df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment this for an example of what one of the sample dataframes should look like\n",
    "#sample_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining sample D\n",
    "concat_df = pd.concat(sample_dfs)\n",
    "\n",
    "# converting some \"object\" dtype columns to strings\n",
    "concat_df[\"kernel_name\"] = concat_df[\"kernel_name\"].astype(str)\n",
    "concat_df[\"op_name\"] = concat_df[\"op_name\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape:  (79115, 18)\n",
      "Columns (and dtypes):\n",
      "rank                             int64\n",
      "kernel_name                     object\n",
      "registers_per_thread             int64\n",
      "shmem_bytes                      int64\n",
      "block_dim                       object\n",
      "grid_dim                        object\n",
      "is_op_tensor_core_eligible        bool\n",
      "is_kernel_using_tensor_core       bool\n",
      "op_name                         object\n",
      "occurrences                      int64\n",
      "total_duration_us              float64\n",
      "avg_duration_us                float64\n",
      "min_duration_us                float64\n",
      "max_duration_us                float64\n",
      "batch_size                       int64\n",
      "stride_length                    int64\n",
      "kernel_size                      int64\n",
      "filter_count                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# printing the schema of our combined DataFrame\n",
    "print(\"DataFrame Shape: \", concat_df.shape)\n",
    "print(\"Columns (and dtypes):\")\n",
    "print(concat_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Records from concat_df:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>kernel_name</th>\n",
       "      <th>registers_per_thread</th>\n",
       "      <th>shmem_bytes</th>\n",
       "      <th>block_dim</th>\n",
       "      <th>grid_dim</th>\n",
       "      <th>is_op_tensor_core_eligible</th>\n",
       "      <th>is_kernel_using_tensor_core</th>\n",
       "      <th>op_name</th>\n",
       "      <th>occurrences</th>\n",
       "      <th>total_duration_us</th>\n",
       "      <th>avg_duration_us</th>\n",
       "      <th>min_duration_us</th>\n",
       "      <th>max_duration_us</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>stride_length</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>filter_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>void fft2d_r2c_32x32&lt;float, false, 0u, false&gt;(...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>gradient_tape/sequential_1020/conv2d_1020/Conv...</td>\n",
       "      <td>2</td>\n",
       "      <td>57.504</td>\n",
       "      <td>28.752</td>\n",
       "      <td>28.736</td>\n",
       "      <td>28.768</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;fl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>sequential_1020/conv2d_1020/Conv2D</td>\n",
       "      <td>1</td>\n",
       "      <td>37.696</td>\n",
       "      <td>37.696</td>\n",
       "      <td>37.696</td>\n",
       "      <td>37.696</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>void gemv2T_kernel_val&lt;int, int, float, float,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>sequential_1020/dense_1020/MatMul</td>\n",
       "      <td>1</td>\n",
       "      <td>14.304</td>\n",
       "      <td>14.304</td>\n",
       "      <td>14.304</td>\n",
       "      <td>14.304</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>void flip_filter&lt;float, float&gt;(float*, float c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>gradient_tape/sequential_1020/conv2d_1020/Conv...</td>\n",
       "      <td>1</td>\n",
       "      <td>13.728</td>\n",
       "      <td>13.728</td>\n",
       "      <td>13.728</td>\n",
       "      <td>13.728</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>void gemv2T_kernel_val&lt;int, int, float2, float...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>gradient_tape/sequential_1020/conv2d_1020/Conv...</td>\n",
       "      <td>1</td>\n",
       "      <td>12.448</td>\n",
       "      <td>12.448</td>\n",
       "      <td>12.448</td>\n",
       "      <td>12.448</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                                        kernel_name  \\\n",
       "0     1  void fft2d_r2c_32x32<float, false, 0u, false>(...   \n",
       "1     2  void cudnn::detail::implicit_convolve_sgemm<fl...   \n",
       "2     3  void gemv2T_kernel_val<int, int, float, float,...   \n",
       "3     4  void flip_filter<float, float>(float*, float c...   \n",
       "4     5  void gemv2T_kernel_val<int, int, float2, float...   \n",
       "\n",
       "   registers_per_thread  shmem_bytes block_dim grid_dim  \\\n",
       "0                     0            0     1,1,1    1,1,1   \n",
       "1                     0            0     1,1,1    1,1,1   \n",
       "2                     0            0     1,1,1    1,1,1   \n",
       "3                     0            0     1,1,1    1,1,1   \n",
       "4                     0            0     1,1,1    1,1,1   \n",
       "\n",
       "   is_op_tensor_core_eligible  is_kernel_using_tensor_core  \\\n",
       "0                        True                        False   \n",
       "1                        True                        False   \n",
       "2                        True                        False   \n",
       "3                        True                        False   \n",
       "4                        True                        False   \n",
       "\n",
       "                                             op_name  occurrences  \\\n",
       "0  gradient_tape/sequential_1020/conv2d_1020/Conv...            2   \n",
       "1                 sequential_1020/conv2d_1020/Conv2D            1   \n",
       "2                  sequential_1020/dense_1020/MatMul            1   \n",
       "3  gradient_tape/sequential_1020/conv2d_1020/Conv...            1   \n",
       "4  gradient_tape/sequential_1020/conv2d_1020/Conv...            1   \n",
       "\n",
       "   total_duration_us  avg_duration_us  min_duration_us  max_duration_us  \\\n",
       "0             57.504           28.752           28.736           28.768   \n",
       "1             37.696           37.696           37.696           37.696   \n",
       "2             14.304           14.304           14.304           14.304   \n",
       "3             13.728           13.728           13.728           13.728   \n",
       "4             12.448           12.448           12.448           12.448   \n",
       "\n",
       "   batch_size  stride_length  kernel_size  filter_count  \n",
       "0         127              1            1             1  \n",
       "1         127              1            1             1  \n",
       "2         127              1            1             1  \n",
       "3         127              1            1             1  \n",
       "4         127              1            1             1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample Records from concat_df:\")\n",
    "concat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the columns in the above DataFrame store information about performance metrics outside the range of what is being studied in this experiment. To make the DataFrame easier to work with, we'll filter out some of this information before continuing further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "filtered_cols = concat_df[[\"kernel_name\", \"op_name\", \"stride_length\", \"filter_count\", \"batch_size\", \n",
    "                           \"total_duration_us\", \"kernel_size\", \"avg_duration_us\", \"min_duration_us\", \"max_duration_us\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out information about kernels other than Conv2D\n",
    "\n",
    "Since the test network was composed of more layers than just the Conv2D (as described in the sampling notebook), our current DataFrame contains a lot of performance information about these other layers. While they are interesting to study, this analysis focuses on the Conv2D layer, so we will filter out the other layers, creating a new DataFrame (conv2d_kernels)\n",
    "\n",
    "Furthermore, it is common that within each layer there are multiple kernels that are used during computation. To study the network on a layer level, we can group these different kernels together (the DataFrame conv2d_layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape:  (1380, 8)\n",
      "Batch Sizes:  [  1   2   3   4   5   7   8   9  15  16  17  31  32  33  63  64  65 127\n",
      " 128 129 319 320 321]\n",
      "Kernel Sizes:  [1 3]\n",
      "Stride Length:  [1 2 3]\n",
      "Filter Counts:  [ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stride_length</th>\n",
       "      <th>filter_count</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>total_duration_us</th>\n",
       "      <th>avg_duration_us</th>\n",
       "      <th>min_duration_us</th>\n",
       "      <th>max_duration_us</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>op_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sequential/conv2d/Conv2D</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.703</td>\n",
       "      <td>20.703</td>\n",
       "      <td>20.703</td>\n",
       "      <td>20.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequential_1/conv2d_1/Conv2D</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.312</td>\n",
       "      <td>21.312</td>\n",
       "      <td>21.312</td>\n",
       "      <td>21.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequential_10/conv2d_10/Conv2D</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24.288</td>\n",
       "      <td>24.288</td>\n",
       "      <td>24.288</td>\n",
       "      <td>24.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequential_100/conv2d_100/Conv2D</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>19.008</td>\n",
       "      <td>19.008</td>\n",
       "      <td>19.008</td>\n",
       "      <td>19.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequential_1000/conv2d_1000/Conv2D</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>20.608</td>\n",
       "      <td>20.608</td>\n",
       "      <td>20.608</td>\n",
       "      <td>20.608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    stride_length  filter_count  kernel_size  \\\n",
       "op_name                                                                        \n",
       "sequential/conv2d/Conv2D                        1             1            1   \n",
       "sequential_1/conv2d_1/Conv2D                    1             2            1   \n",
       "sequential_10/conv2d_10/Conv2D                  1             1            3   \n",
       "sequential_100/conv2d_100/Conv2D                3             1            1   \n",
       "sequential_1000/conv2d_1000/Conv2D              3             1            1   \n",
       "\n",
       "                                    batch_size  total_duration_us  \\\n",
       "op_name                                                             \n",
       "sequential/conv2d/Conv2D                     1             20.703   \n",
       "sequential_1/conv2d_1/Conv2D                 1             21.312   \n",
       "sequential_10/conv2d_10/Conv2D               1             24.288   \n",
       "sequential_100/conv2d_100/Conv2D             2             19.008   \n",
       "sequential_1000/conv2d_1000/Conv2D          65             20.608   \n",
       "\n",
       "                                    avg_duration_us  min_duration_us  \\\n",
       "op_name                                                                \n",
       "sequential/conv2d/Conv2D                     20.703           20.703   \n",
       "sequential_1/conv2d_1/Conv2D                 21.312           21.312   \n",
       "sequential_10/conv2d_10/Conv2D               24.288           24.288   \n",
       "sequential_100/conv2d_100/Conv2D             19.008           19.008   \n",
       "sequential_1000/conv2d_1000/Conv2D           20.608           20.608   \n",
       "\n",
       "                                    max_duration_us  \n",
       "op_name                                              \n",
       "sequential/conv2d/Conv2D                     20.703  \n",
       "sequential_1/conv2d_1/Conv2D                 21.312  \n",
       "sequential_10/conv2d_10/Conv2D               24.288  \n",
       "sequential_100/conv2d_100/Conv2D             19.008  \n",
       "sequential_1000/conv2d_1000/Conv2D           20.608  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_kernels = filtered_cols[filtered_cols[\"op_name\"].str.endswith(\"Conv2D\")].copy()\n",
    "\n",
    "conv2d_layers = conv2d_kernels.groupby(\n",
    "    by=[\"op_name\", \"stride_length\", \"filter_count\", \"kernel_size\" ,\"batch_size\"]).sum().reset_index().copy()\n",
    "\n",
    "# moving op_name back to the index\n",
    "conv2d_layers.set_index(\"op_name\", inplace=True)\n",
    "conv2d_kernels.reset_index(inplace=True)\n",
    "conv2d_kernels.set_index(\"op_name\", inplace=True)\n",
    "conv2d_kernels.drop([\"index\"], axis=1, inplace=True)\n",
    "\n",
    "# doing  quick verification that everything turned out alright\n",
    "\n",
    "# should contain 1380 unique rows\n",
    "print(\"Output Shape: \", conv2d_layers.shape)\n",
    "\n",
    "# Checking that all parameters made it into the filtered DataFrame\n",
    "print(\"Batch Sizes: \", np.sort(conv2d_layers.batch_size.unique()))\n",
    "print(\"Kernel Sizes: \", np.sort(conv2d_layers.kernel_size.unique()))\n",
    "print(\"Stride Length: \", np.sort(conv2d_layers.stride_length.unique()))\n",
    "print(\"Filter Counts: \", np.sort(conv2d_layers.filter_count.unique()))\n",
    "\n",
    "conv2d_layers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table output above, it seems like the total, min, avg and max duration columns all contain the same value for every row. Let's see if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total and Avg Same:  True\n",
      "Total and Max Same:  True\n",
      "Total and Min Same:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Total and Avg Same: \", (conv2d_layers[\"total_duration_us\"] == conv2d_layers[\"avg_duration_us\"]).all())\n",
    "print(\"Total and Max Same: \",(conv2d_layers[\"total_duration_us\"] == conv2d_layers[\"max_duration_us\"]).all())\n",
    "print(\"Total and Min Same: \",(conv2d_layers[\"total_duration_us\"] == conv2d_layers[\"min_duration_us\"]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total and Avg Same:  True\n",
      "Total and Max Same:  True\n",
      "Total and Min Same:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Total and Avg Same: \", (conv2d_kernels[\"total_duration_us\"] == conv2d_kernels[\"avg_duration_us\"]).all())\n",
    "print(\"Total and Max Same: \",(conv2d_kernels[\"total_duration_us\"] == conv2d_kernels[\"max_duration_us\"]).all())\n",
    "print(\"Total and Min Same: \",(conv2d_kernels[\"total_duration_us\"] == conv2d_kernels[\"min_duration_us\"]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we only need to keep one copy of this column. Intuitively, this should make sense since we sampled for a single run. (Do this for both the kernels and layers DataFrames)\n",
    "\n",
    "(Note, we could also do a supporting verification of this fact by looking at the \"Occurences\" column in the original DataFrame and noticing that no Conv2D kernels have a value greater than 1 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_layers.drop([\"avg_duration_us\", \"max_duration_us\", \"min_duration_us\"], axis =1, inplace=True)\n",
    "conv2d_kernels.drop([\"avg_duration_us\", \"max_duration_us\", \"min_duration_us\"], axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the DataFrame for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the DataFrame as a CSV, since it is easy to do with Pandas\n",
    "# the string data_location is the directory containing the export_logs folder as used above\n",
    "conv2d_kernels.to_csv(path_or_buf = (data_location + \"conv2d_kernels.csv\"))\n",
    "conv2d_layers.to_csv(path_or_buf = (data_location + \"conv2d_layers.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
